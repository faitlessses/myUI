{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experiment: Flux LoRA Vast.ai ai-toolkit Workflow\n",
        "\n",
        "Objective:\n",
        "- Train a personal Flux LoRA end-to-end on Vast.ai with reproducible controls.\n",
        "- Keep all operational knobs in one place for fast iteration on rented GPUs.\n",
        "\n",
        "Success criteria:\n",
        "- Dataset passes quality checks.\n",
        "- Training launches (or resumes) from generated config.\n",
        "- Final artifacts are bundled for download.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import json\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "import tarfile\n",
        "import textwrap\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "def run(cmd: list[str], cwd: str | None = None, check: bool = True) -> subprocess.CompletedProcess:\n",
        "    print(\"$\", \" \".join(cmd))\n",
        "    return subprocess.run(cmd, cwd=cwd, check=check, text=True, capture_output=False)\n",
        "\n",
        "\n",
        "def run_capture(cmd: list[str], cwd: str | None = None) -> str:\n",
        "    print(\"$\", \" \".join(cmd))\n",
        "    out = subprocess.check_output(cmd, cwd=cwd, text=True)\n",
        "    return out.strip()\n",
        "\n",
        "\n",
        "def ensure_package(package: str) -> None:\n",
        "    try:\n",
        "        __import__(package)\n",
        "    except Exception:\n",
        "        run([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "print(\"Python:\", sys.version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prerequisites\n",
        "\n",
        "- Vast.ai Linux instance with NVIDIA GPU.\n",
        "- Dataset available as folder or zip.\n",
        "- Permission to train the base model and dataset content.\n",
        "- You are responsible for legal/TOS compliance, especially if training NSFW content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Central configuration (edit this cell first)\n",
        "PROJECT_NAME = \"my_flux_lora\"\n",
        "BASE_MODEL_ID = \"black-forest-labs/FLUX.1-dev\"\n",
        "\n",
        "# Paths: auto-detect common Vast mount points, then fallback.\n",
        "VAST_AUTO_MOUNT_PATHS = [\n",
        "    \"/workspace\",\n",
        "    \"/root/autodl-tmp\",\n",
        "    \"/data\",\n",
        "]\n",
        "USER_WORKSPACE_FALLBACK = \"/workspace\"\n",
        "\n",
        "# Dataset input\n",
        "DATASET_ZIP = \"\"   # e.g. /workspace/datasets/myset.zip\n",
        "DATASET_DIR = \"\"   # e.g. /workspace/datasets/myset (if already extracted)\n",
        "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".webp\"}\n",
        "\n",
        "# Captioning\n",
        "AUTO_CAPTION = False\n",
        "CAPTION_MODEL = \"nlpconnect/vit-gpt2-image-captioning\"\n",
        "CAPTION_FILE_EXT = \".txt\"\n",
        "\n",
        "# Safety and policy knob\n",
        "ALLOW_NSFW = True\n",
        "\n",
        "# Quality gate\n",
        "QUALITY_GATE = True\n",
        "MIN_IMAGE_COUNT = 20\n",
        "MIN_RESOLUTION = 512\n",
        "REQUIRE_CAPTIONS = True\n",
        "\n",
        "# Training knobs\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 1\n",
        "GRAD_ACCUM = 4\n",
        "NETWORK_DIM = 32\n",
        "NETWORK_ALPHA = 16\n",
        "MIXED_PRECISION = \"bf16\"\n",
        "SAVE_EVERY_N_STEPS = 200\n",
        "MAX_TRAIN_STEPS = 2000\n",
        "\n",
        "# Resume support\n",
        "RESUME_FROM_CHECKPOINT = \"\"   # path to checkpoint; empty means fresh training\n",
        "\n",
        "# Logging\n",
        "WANDB_ENABLED = False\n",
        "WANDB_PROJECT = \"flux-lora\"\n",
        "WANDB_RUN_NAME = PROJECT_NAME\n",
        "\n",
        "# Cost guard (estimate only)\n",
        "COST_GUARD = True\n",
        "GPU_HOURLY_USD = 0.55\n",
        "ASSUMED_STEPS_PER_SEC = 0.65\n",
        "\n",
        "# ai-toolkit repo\n",
        "AI_TOOLKIT_REPO = \"https://github.com/ostris/ai-toolkit.git\"\n",
        "AI_TOOLKIT_DIRNAME = \"ai-toolkit\"\n",
        "TRAIN_COMMAND_OVERRIDE = \"\"  # optional full command string\n",
        "\n",
        "assert ALLOW_NSFW in {True, False}\n",
        "print(\"Config loaded for:\", PROJECT_NAME)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resolve workspace and project paths\n",
        "mount_root = None\n",
        "for candidate in VAST_AUTO_MOUNT_PATHS:\n",
        "    if Path(candidate).exists():\n",
        "        mount_root = Path(candidate)\n",
        "        break\n",
        "\n",
        "if mount_root is None:\n",
        "    mount_root = Path(USER_WORKSPACE_FALLBACK)\n",
        "\n",
        "ROOT = mount_root\n",
        "WORKDIR = ROOT / PROJECT_NAME\n",
        "DATA_DIR = WORKDIR / \"data\"\n",
        "TRAIN_DIR = WORKDIR / \"train\"\n",
        "CAPTION_DIR = TRAIN_DIR / \"captions\"\n",
        "OUTPUT_DIR = WORKDIR / \"outputs\"\n",
        "LOG_DIR = WORKDIR / \"logs\"\n",
        "CONFIG_DIR = WORKDIR / \"config\"\n",
        "PUBLISH_DIR = WORKDIR / \"publish\"\n",
        "\n",
        "for p in [WORKDIR, DATA_DIR, TRAIN_DIR, CAPTION_DIR, OUTPUT_DIR, LOG_DIR, CONFIG_DIR, PUBLISH_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"ROOT:\", ROOT)\n",
        "print(\"WORKDIR:\", WORKDIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Runtime preflight\n",
        "print(\"== Runtime preflight ==\")\n",
        "\n",
        "try:\n",
        "    run([\"nvidia-smi\"])\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"nvidia-smi failed. Confirm GPU-enabled Vast instance.\") from e\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(\"torch:\", torch.__version__)\n",
        "    print(\"cuda available:\", torch.cuda.is_available())\n",
        "    if not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA unavailable in torch runtime.\")\n",
        "    print(\"gpu:\", torch.cuda.get_device_name(0))\n",
        "    total_mem_gb = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)\n",
        "    print(f\"vram_gb: {total_mem_gb:.2f}\")\n",
        "except ImportError:\n",
        "    print(\"torch not installed yet; bootstrap cell will install dependencies.\")\n",
        "\n",
        "usage = shutil.disk_usage(str(ROOT))\n",
        "free_gb = usage.free / (1024 ** 3)\n",
        "print(f\"disk_free_gb: {free_gb:.2f}\")\n",
        "if free_gb < 20:\n",
        "    raise RuntimeError(\"Less than 20GB free disk. Increase storage before training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment bootstrap\n",
        "AI_TOOLKIT_DIR = ROOT / AI_TOOLKIT_DIRNAME\n",
        "VENV_DIR = ROOT / \".venv_flux\"\n",
        "\n",
        "if not AI_TOOLKIT_DIR.exists():\n",
        "    run([\"git\", \"clone\", AI_TOOLKIT_REPO, str(AI_TOOLKIT_DIR)])\n",
        "else:\n",
        "    run([\"git\", \"-C\", str(AI_TOOLKIT_DIR), \"pull\", \"--ff-only\"])\n",
        "\n",
        "if not VENV_DIR.exists():\n",
        "    run([sys.executable, \"-m\", \"venv\", str(VENV_DIR)])\n",
        "\n",
        "PYTHON_BIN = str(VENV_DIR / \"bin\" / \"python\")\n",
        "PIP_BIN = [PYTHON_BIN, \"-m\", \"pip\"]\n",
        "\n",
        "run(PIP_BIN + [\"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"])\n",
        "req = AI_TOOLKIT_DIR / \"requirements.txt\"\n",
        "if req.exists():\n",
        "    run(PIP_BIN + [\"install\", \"-r\", str(req)])\n",
        "else:\n",
        "    print(\"requirements.txt not found; install your fork dependencies manually if needed.\")\n",
        "\n",
        "run([PYTHON_BIN, \"-c\", \"import yaml; print('pyyaml ok')\"])\n",
        "print(\"Bootstrap complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Entrypoint Diagnostics\n",
        "\n",
        "Run this once after bootstrap. It detects likely ai-toolkit train/inference entrypoints and prints copy-paste commands for your fork.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Entrypoint diagnostics\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"AI_TOOLKIT_DIR:\", AI_TOOLKIT_DIR)\n",
        "print(\"PYTHON_BIN:\", PYTHON_BIN)\n",
        "\n",
        "candidates = [\n",
        "    \"toolkit.train\",\n",
        "    \"ai_toolkit.train\",\n",
        "    \"toolkit.inference\",\n",
        "    \"ai_toolkit.inference\",\n",
        "]\n",
        "\n",
        "print(\"\\nModule availability:\")\n",
        "for mod in candidates:\n",
        "    cmd = f\"{PYTHON_BIN} -c \\\"import importlib.util; print(importlib.util.find_spec('{mod}') is not None)\\\"\"\n",
        "    ok = subprocess.check_output(cmd, shell=True, text=True).strip()\n",
        "    print(f\"- {mod}: {ok}\")\n",
        "\n",
        "print(\"\\nCommon script files in repo root:\")\n",
        "for name in [\"run.py\", \"train.py\", \"inference.py\", \"main.py\"]:\n",
        "    p = AI_TOOLKIT_DIR / name\n",
        "    print(f\"- {name}:\", p.exists())\n",
        "\n",
        "print(\"\\nSuggested train commands:\")\n",
        "print(f\"1) {PYTHON_BIN} -m toolkit.train {config_path}\")\n",
        "print(f\"2) {PYTHON_BIN} -m ai_toolkit.train {config_path}\")\n",
        "print(f\"3) {PYTHON_BIN} run.py train {config_path}\")\n",
        "print(f\"4) {PYTHON_BIN} train.py {config_path}\")\n",
        "\n",
        "print(\"\\nSuggested inference commands:\")\n",
        "print(f\"1) {PYTHON_BIN} -m toolkit.inference --base {BASE_MODEL_ID} --lora <lora.safetensors> --prompt 'portrait, cinematic lighting' --out {OUTPUT_DIR / 'samples'}\")\n",
        "print(f\"2) {PYTHON_BIN} -m ai_toolkit.inference --base {BASE_MODEL_ID} --lora <lora.safetensors> --prompt 'portrait, cinematic lighting' --out {OUTPUT_DIR / 'samples'}\")\n",
        "print(f\"3) {PYTHON_BIN} inference.py --base {BASE_MODEL_ID} --lora <lora.safetensors> --prompt 'portrait, cinematic lighting' --out {OUTPUT_DIR / 'samples'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset and captions\n",
        "\n",
        "This section ingests your dataset, creates caption sidecars, and checks quality before any training spend.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset ingest and normalization\n",
        "import zipfile\n",
        "\n",
        "if DATASET_ZIP:\n",
        "    zsrc = Path(DATASET_ZIP)\n",
        "    if not zsrc.exists():\n",
        "        raise FileNotFoundError(f\"DATASET_ZIP not found: {zsrc}\")\n",
        "    with zipfile.ZipFile(zsrc, \"r\") as zf:\n",
        "        zf.extractall(DATA_DIR)\n",
        "\n",
        "if DATASET_DIR:\n",
        "    src = Path(DATASET_DIR)\n",
        "    if not src.exists():\n",
        "        raise FileNotFoundError(f\"DATASET_DIR not found: {src}\")\n",
        "    for item in src.rglob(\"*\"):\n",
        "        if item.is_file() and item.suffix.lower() in IMAGE_EXTS:\n",
        "            dst = TRAIN_DIR / item.name\n",
        "            if not dst.exists():\n",
        "                shutil.copy2(item, dst)\n",
        "else:\n",
        "    for item in DATA_DIR.rglob(\"*\"):\n",
        "        if item.is_file() and item.suffix.lower() in IMAGE_EXTS:\n",
        "            dst = TRAIN_DIR / item.name\n",
        "            if not dst.exists():\n",
        "                shutil.copy2(item, dst)\n",
        "\n",
        "images = sorted([p for p in TRAIN_DIR.iterdir() if p.is_file() and p.suffix.lower() in IMAGE_EXTS])\n",
        "print(\"images_found:\", len(images))\n",
        "if not images:\n",
        "    raise RuntimeError(\"No images found. Set DATASET_ZIP or DATASET_DIR correctly.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Captioning (optional auto-caption, always manual-editable)\n",
        "if AUTO_CAPTION:\n",
        "    from PIL import Image\n",
        "    from transformers import pipeline\n",
        "\n",
        "    cap = pipeline(\"image-to-text\", model=CAPTION_MODEL)\n",
        "    for img_path in images:\n",
        "        txt_path = CAPTION_DIR / f\"{img_path.stem}{CAPTION_FILE_EXT}\"\n",
        "        if txt_path.exists():\n",
        "            continue\n",
        "        out = cap(Image.open(img_path))\n",
        "        text = out[0].get(\"generated_text\", \"\").strip()\n",
        "        txt_path.write_text(text + \"\\n\", encoding=\"utf-8\")\n",
        "else:\n",
        "    for img_path in images:\n",
        "        txt_path = CAPTION_DIR / f\"{img_path.stem}{CAPTION_FILE_EXT}\"\n",
        "        if not txt_path.exists():\n",
        "            # Starter caption for manual editing\n",
        "            txt_path.write_text(f\"{PROJECT_NAME}, subject, high detail\\n\", encoding=\"utf-8\")\n",
        "\n",
        "print(\"caption_files:\", len(list(CAPTION_DIR.glob(f\"*{CAPTION_FILE_EXT}\"))))\n",
        "print(\"Edit caption files in:\", CAPTION_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quality gate\n",
        "from PIL import Image\n",
        "\n",
        "if QUALITY_GATE:\n",
        "    if len(images) < MIN_IMAGE_COUNT:\n",
        "        raise RuntimeError(f\"Quality gate failed: {len(images)} images < MIN_IMAGE_COUNT({MIN_IMAGE_COUNT})\")\n",
        "\n",
        "    too_small = []\n",
        "    for p in images:\n",
        "        w, h = Image.open(p).size\n",
        "        if min(w, h) < MIN_RESOLUTION:\n",
        "            too_small.append((p.name, w, h))\n",
        "\n",
        "    caption_files = list(CAPTION_DIR.glob(f\"*{CAPTION_FILE_EXT}\"))\n",
        "    caption_names = {p.stem for p in caption_files}\n",
        "    missing_caps = [p.name for p in images if p.stem not in caption_names]\n",
        "\n",
        "    print(\"too_small_count:\", len(too_small))\n",
        "    print(\"missing_caption_count:\", len(missing_caps))\n",
        "\n",
        "    if too_small:\n",
        "        raise RuntimeError(f\"Quality gate failed: {len(too_small)} images below MIN_RESOLUTION={MIN_RESOLUTION}\")\n",
        "    if REQUIRE_CAPTIONS and missing_caps:\n",
        "        raise RuntimeError(f\"Quality gate failed: {len(missing_caps)} missing captions\")\n",
        "\n",
        "print(\"Quality gate passed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate training config (YAML)\n",
        "import yaml\n",
        "\n",
        "config = {\n",
        "    \"job\": {\n",
        "        \"name\": PROJECT_NAME,\n",
        "        \"type\": \"lora\",\n",
        "    },\n",
        "    \"meta\": {\n",
        "        \"allow_nsfw\": ALLOW_NSFW,\n",
        "        \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    },\n",
        "    \"model\": {\n",
        "        \"base_model\": BASE_MODEL_ID,\n",
        "    },\n",
        "    \"data\": {\n",
        "        \"train_images\": str(TRAIN_DIR),\n",
        "        \"captions\": str(CAPTION_DIR),\n",
        "    },\n",
        "    \"train\": {\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"grad_accum\": GRAD_ACCUM,\n",
        "        \"save_every_n_steps\": SAVE_EVERY_N_STEPS,\n",
        "        \"max_train_steps\": MAX_TRAIN_STEPS,\n",
        "        \"mixed_precision\": MIXED_PRECISION,\n",
        "    },\n",
        "    \"network\": {\n",
        "        \"dim\": NETWORK_DIM,\n",
        "        \"alpha\": NETWORK_ALPHA,\n",
        "    },\n",
        "    \"output\": {\n",
        "        \"dir\": str(OUTPUT_DIR),\n",
        "        \"logs\": str(LOG_DIR),\n",
        "    },\n",
        "}\n",
        "\n",
        "if RESUME_FROM_CHECKPOINT:\n",
        "    config[\"train\"][\"resume_from_checkpoint\"] = RESUME_FROM_CHECKPOINT\n",
        "\n",
        "if WANDB_ENABLED:\n",
        "    config[\"logging\"] = {\n",
        "        \"wandb\": {\n",
        "            \"project\": WANDB_PROJECT,\n",
        "            \"run_name\": WANDB_RUN_NAME,\n",
        "        }\n",
        "    }\n",
        "\n",
        "config_path = CONFIG_DIR / \"train_flux_lora.yaml\"\n",
        "config_path.write_text(yaml.safe_dump(config, sort_keys=False), encoding=\"utf-8\")\n",
        "print(\"Wrote:\", config_path)\n",
        "print(config_path.read_text(encoding=\"utf-8\")[:1200])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cost guard estimate\n",
        "if COST_GUARD:\n",
        "    est_steps = MAX_TRAIN_STEPS\n",
        "    est_seconds = est_steps / max(ASSUMED_STEPS_PER_SEC, 1e-6)\n",
        "    est_hours = est_seconds / 3600\n",
        "    est_usd = est_hours * GPU_HOURLY_USD\n",
        "    print(f\"estimated_steps: {est_steps}\")\n",
        "    print(f\"estimated_hours: {est_hours:.2f}\")\n",
        "    print(f\"estimated_cost_usd: {est_usd:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Launch training\n",
        "train_log = LOG_DIR / \"train.log\"\n",
        "\n",
        "if RESUME_FROM_CHECKPOINT and not Path(RESUME_FROM_CHECKPOINT).exists():\n",
        "    raise FileNotFoundError(f\"RESUME_FROM_CHECKPOINT not found: {RESUME_FROM_CHECKPOINT}\")\n",
        "\n",
        "def build_candidate_commands() -> list[str]:\n",
        "    if TRAIN_COMMAND_OVERRIDE.strip():\n",
        "        return [TRAIN_COMMAND_OVERRIDE.strip()]\n",
        "\n",
        "    candidates = [\n",
        "        f\"{PYTHON_BIN} -m toolkit.train {config_path}\",\n",
        "        f\"{PYTHON_BIN} -m ai_toolkit.train {config_path}\",\n",
        "        f\"{PYTHON_BIN} run.py train {config_path}\",\n",
        "        f\"{PYTHON_BIN} train.py {config_path}\",\n",
        "    ]\n",
        "    return candidates\n",
        "\n",
        "candidates = build_candidate_commands()\n",
        "print(\"candidate_train_commands:\")\n",
        "for c in candidates:\n",
        "    print(\" -\", c)\n",
        "\n",
        "chosen_command = None\n",
        "if TRAIN_COMMAND_OVERRIDE.strip():\n",
        "    chosen_command = candidates[0]\n",
        "else:\n",
        "    for cmd in candidates:\n",
        "        # Cheap pre-check for module entrypoints before full launch\n",
        "        if \"-m toolkit.train\" in cmd:\n",
        "            check_cmd = f\"{PYTHON_BIN} -c \\\"import importlib.util; print(importlib.util.find_spec('toolkit.train') is not None)\\\"\"\n",
        "            ok = subprocess.check_output(check_cmd, shell=True, text=True).strip().endswith(\"True\")\n",
        "            if ok:\n",
        "                chosen_command = cmd\n",
        "                break\n",
        "            continue\n",
        "        if \"-m ai_toolkit.train\" in cmd:\n",
        "            check_cmd = f\"{PYTHON_BIN} -c \\\"import importlib.util; print(importlib.util.find_spec('ai_toolkit.train') is not None)\\\"\"\n",
        "            ok = subprocess.check_output(check_cmd, shell=True, text=True).strip().endswith(\"True\")\n",
        "            if ok:\n",
        "                chosen_command = cmd\n",
        "                break\n",
        "            continue\n",
        "        # Script fallback\n",
        "        token = cmd.split()[1] if len(cmd.split()) > 1 else \"\"\n",
        "        if token and (AI_TOOLKIT_DIR / token).exists():\n",
        "            chosen_command = cmd\n",
        "            break\n",
        "\n",
        "if not chosen_command:\n",
        "    raise RuntimeError(\n",
        "        \"No known ai-toolkit train entrypoint detected. Set TRAIN_COMMAND_OVERRIDE explicitly, \"\n",
        "        \"e.g. '<venv_python> -m toolkit.train <config_path>'\"\n",
        "    )\n",
        "\n",
        "print(\"train_command:\", chosen_command)\n",
        "print(\"log_file:\", train_log)\n",
        "\n",
        "with open(train_log, \"a\", encoding=\"utf-8\") as lf:\n",
        "    lf.write(f\"\\n\\n===== START {datetime.utcnow().isoformat()}Z =====\\n\")\n",
        "    proc = subprocess.Popen(chosen_command, shell=True, cwd=str(AI_TOOLKIT_DIR), stdout=lf, stderr=lf)\n",
        "\n",
        "print(\"PID:\", proc.pid)\n",
        "print(\"Use the next cell to tail logs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tail latest logs (run repeatedly)\n",
        "train_log = LOG_DIR / \"train.log\"\n",
        "if not train_log.exists():\n",
        "    raise FileNotFoundError(f\"No log file yet: {train_log}\")\n",
        "\n",
        "lines = train_log.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
        "for line in lines[-80:]:\n",
        "    print(line)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validation / quick inference hook\n",
        "# Auto-detect common inference entrypoints; fallback to TRAIN_COMMAND_OVERRIDE style manual override.\n",
        "SAMPLES_DIR = OUTPUT_DIR / \"samples\"\n",
        "SAMPLES_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "final_loras = sorted(OUTPUT_DIR.rglob(\"*.safetensors\"), key=lambda p: p.stat().st_mtime)\n",
        "if not final_loras:\n",
        "    print(\"No LoRA artifact yet. Finish training before inference.\")\n",
        "else:\n",
        "    final_lora = final_loras[-1]\n",
        "    prompt = f\"{PROJECT_NAME}, cinematic portrait, high detail\"\n",
        "\n",
        "    candidates = [\n",
        "        f\"{PYTHON_BIN} -m toolkit.inference --base {BASE_MODEL_ID} --lora {final_lora} --prompt \\\"{prompt}\\\" --out {SAMPLES_DIR}\",\n",
        "        f\"{PYTHON_BIN} -m ai_toolkit.inference --base {BASE_MODEL_ID} --lora {final_lora} --prompt \\\"{prompt}\\\" --out {SAMPLES_DIR}\",\n",
        "        f\"{PYTHON_BIN} inference.py --base {BASE_MODEL_ID} --lora {final_lora} --prompt \\\"{prompt}\\\" --out {SAMPLES_DIR}\",\n",
        "    ]\n",
        "\n",
        "    chosen = None\n",
        "    for cmd in candidates:\n",
        "        if \"-m toolkit.inference\" in cmd:\n",
        "            check_cmd = f\"{PYTHON_BIN} -c \\\"import importlib.util; print(importlib.util.find_spec('toolkit.inference') is not None)\\\"\"\n",
        "            ok = subprocess.check_output(check_cmd, shell=True, text=True).strip().endswith(\"True\")\n",
        "            if ok:\n",
        "                chosen = cmd\n",
        "                break\n",
        "            continue\n",
        "        if \"-m ai_toolkit.inference\" in cmd:\n",
        "            check_cmd = f\"{PYTHON_BIN} -c \\\"import importlib.util; print(importlib.util.find_spec('ai_toolkit.inference') is not None)\\\"\"\n",
        "            ok = subprocess.check_output(check_cmd, shell=True, text=True).strip().endswith(\"True\")\n",
        "            if ok:\n",
        "                chosen = cmd\n",
        "                break\n",
        "            continue\n",
        "        if \"inference.py\" in cmd and (AI_TOOLKIT_DIR / \"inference.py\").exists():\n",
        "            chosen = cmd\n",
        "            break\n",
        "\n",
        "    if chosen:\n",
        "        print(\"Running inference:\", chosen)\n",
        "        run([\"bash\", \"-lc\", chosen], cwd=str(AI_TOOLKIT_DIR), check=False)\n",
        "        print(\"Samples dir:\", SAMPLES_DIR)\n",
        "    else:\n",
        "        print(\"No known inference entrypoint detected.\")\n",
        "        print(\"Set a manual inference command in this cell for your fork.\")\n",
        "        print(\"Example:\")\n",
        "        print(f\"{PYTHON_BIN} -m toolkit.inference --base {BASE_MODEL_ID} --lora {final_lora} --prompt 'portrait, cinematic lighting' --out {SAMPLES_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Publish pack: LoRA + metadata + prompts\n",
        "final_loras = sorted(OUTPUT_DIR.rglob(\"*.safetensors\"), key=lambda p: p.stat().st_mtime)\n",
        "if not final_loras:\n",
        "    raise RuntimeError(f\"No .safetensors found under {OUTPUT_DIR}. Finish training first.\")\n",
        "\n",
        "final_lora = final_loras[-1]\n",
        "metadata = {\n",
        "    \"project_name\": PROJECT_NAME,\n",
        "    \"base_model\": BASE_MODEL_ID,\n",
        "    \"allow_nsfw\": ALLOW_NSFW,\n",
        "    \"final_lora\": str(final_lora),\n",
        "    \"created_at\": datetime.utcnow().isoformat() + \"Z\",\n",
        "    \"recommended_prompts\": [\n",
        "        f\"{PROJECT_NAME}, detailed portrait, high contrast\",\n",
        "        f\"{PROJECT_NAME}, cinematic scene, volumetric lighting\",\n",
        "    ],\n",
        "    \"negative_prompt\": \"low quality, blurry, artifacts\",\n",
        "}\n",
        "\n",
        "meta_path = PUBLISH_DIR / \"metadata.json\"\n",
        "meta_path.write_text(json.dumps(metadata, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "bundle_path = PUBLISH_DIR / f\"{PROJECT_NAME}-publish-pack.tar.gz\"\n",
        "with tarfile.open(bundle_path, \"w:gz\") as tf:\n",
        "    tf.add(final_lora, arcname=final_lora.name)\n",
        "    tf.add(config_path, arcname=\"train_flux_lora.yaml\")\n",
        "    tf.add(meta_path, arcname=\"metadata.json\")\n",
        "\n",
        "print(\"final_lora:\", final_lora)\n",
        "print(\"publish_bundle:\", bundle_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next steps\n",
        "\n",
        "- Re-run only changed cells when tuning hyperparameters.\n",
        "- Keep each run in a new `PROJECT_NAME` to preserve logs and checkpoints.\n",
        "- If your ai-toolkit fork uses different commands, edit `TRAIN_COMMAND_OVERRIDE` and inference cell.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
